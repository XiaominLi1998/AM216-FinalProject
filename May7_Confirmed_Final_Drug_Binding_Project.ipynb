{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "May7_Confirmed_Final_Drug_Binding_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN6dxHwuSFXW"
      },
      "source": [
        "# AM216 Final-Project: Extend Mini-Project: Predicting drug target interaction using 3D information and predicting ligands that can bind to COVID-19 targets.\n",
        "\n",
        "Components of X:\n",
        "*   **Protein**: residues chains (vector length 1000 or other lengths we choose to truncate)\n",
        "*   **Protein 3D**: contact map of residue distances (vector length around ~20000 after flattening the triangular matrix; need to standardize)\n",
        "*   **Ligand**: ECFP signature (1024 or 2048)\n",
        "\n",
        "y: affinity score\n",
        "\n",
        "Proposed models:\n",
        "1.   **Mini project**: concatenate protein residue chain and ligant signature to get X\n",
        "2.   **Just contact map**: concatenate flattened contact map and ligand signature to get X\n",
        "3.   **Early fusion**: concatenate protein residue chain, flattened contact map and ligand signature to get X\n",
        "4.   **Late fusion**: train and predict using Model 1 and Model 2 separately, consensus model at the end\n",
        "5.   **CNN**: Instead of flattening contact map, use convolutaional NN to convolve the map and derive flattened layer\n",
        "6.   **Pocket**: Instead of using the contact map of the whole protein, get the residues of the just the pocket and get the contact map\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVhNjK4kSFXZ"
      },
      "source": [
        "## 1. Install Packages\n",
        "\n",
        "*   Rdkit\n",
        "*   Deepchem\n",
        "* Lifelines (calculate CI scores faster)\n",
        "* Biopython (BioPython's **Bio.PDB** module includes code to load PDB files and calculate the alpha carbon distance between pair of their residues)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7fQCZJ8q9Je"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps7x414nDcHu",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "bbaae7b3-3628-4cd0-f072-9c1acce41ed3"
      },
      "source": [
        "%%capture cap --no-stderr \n",
        "#%%capture is used to suppress the output of this particular cell \n",
        "\n",
        "import sys\n",
        "import time\n",
        "import os.path\n",
        "if not os.path.exists('Anaconda3-2019.10-Linux-x86_64.sh'):\n",
        "  t_start = time.time()\n",
        "  !wget -c https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh\n",
        "  !chmod +x Anaconda3-2019.10-Linux-x86_64.sh\n",
        "  !bash ./Anaconda3-2019.10-Linux-x86_64.sh -b -f -p /usr/local\n",
        "  print(\"Done: Anaconda3\", file=sys.stderr)\n",
        "  t_end = time.time(); print(\"----Elasped: %f seconds\" % (t_end - t_start), file=sys.stderr); t_start = time.time();\n",
        "\n",
        "\n",
        "  !conda install -y -c deepchem -c rdkit -c conda-forge -c omnia deepchem-gpu=2.3.0\n",
        "  import deepchem as dc\n",
        "  print(\"Done: deepchem & rdkit\", file=sys.stderr)\n",
        "  # !conda create -c conda-forge -n my-rdkit-env rdkit;\n",
        "  # !activate my-rdkit-env; #(OR TRY: !activate my-rdkit-env)\n",
        "  # !conda install -c conda-forge rdkit;\n",
        "  # print(\"Done: rdkit\", file=sys.stderr)\n",
        "  t_end = time.time(); print(\"----Elasped: %f seconds\" % (t_end - t_start), file=sys.stderr); t_start = time.time();\n",
        "\n",
        "  !pip install lifelines;\n",
        "  !pip install biopython;\n",
        "  !pip install mmtf-python;\n",
        "  print(\"Done: pip install lifelines & biopython & mmtf-python\", file=sys.stderr)\n",
        "  t_end = time.time(); print(\"----Elasped: %f seconds\" % (t_end - t_start), file=sys.stderr);"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done: Anaconda3\n",
            "----Elasped: 116.495961 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-965fdd005a77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conda install -y -c deepchem -c rdkit -c conda-forge -c omnia deepchem-gpu=2.3.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0;32mimport\u001b[0m \u001b[0mdeepchem\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done: deepchem & rdkit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m# !conda create -c conda-forge -n my-rdkit-env rdkit;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deepchem'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcdYdzPuHNfH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "d7c50e50-6bfd-41c6-a955-079c3f9d372d"
      },
      "source": [
        "%%capture\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import os.path\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "import Bio.PDB\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from rdkit.Chem import AllChem as Chem\n",
        "from rdkit.Chem import MolFromSmiles\n",
        "from Bio.PDB.mmtf import MMTFParser\n",
        "\n",
        "  \n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import pickle\n",
        "import networkx as nx\n",
        "from collections import OrderedDict\n",
        "from lifelines.utils import concordance_index as ci"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-2080961bccbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mBio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPDB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Bio'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Cl4Ai8CSFYE"
      },
      "source": [
        "## 2. Parsing Data\n",
        "TODO: describe the data we are using.\n",
        "\n",
        "### 2.1 Parsing PDB data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHNSbOVERrb_"
      },
      "source": [
        "\n",
        "\n",
        "pdb_data = pd.read_csv('PDBbind2019PLdata.csv')\n",
        "pdb_proteinName = np.array(pdb_data['proteinName'])\n",
        "pdb_affinity = np.array(pdb_data['affinity'])\n",
        "pdb_ligandName = np.array(pdb_data['ligandName'])\n",
        "\n",
        "\n",
        "\n",
        "pdb_data_small = pd.read_csv('pdbbind_core_df_cleaned.csv')\n",
        "pdb_proteinName_small = np.array(pdb_data_small['proteinName'])\n",
        "pdb_affinity_small = np.array(pdb_data_small['affinity'])\n",
        "pdb_ligandName_small = np.array(pdb_data_small['ligandName'])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rpxK-GhRuFF",
        "outputId": "658a0129-17e6-4424-c029-d08b357d4a74"
      },
      "source": [
        "print(\"pdb_proteinName.shape:\", pdb_proteinName.shape)\n",
        "print(\"pdb_affinity.shape:\", pdb_affinity.shape)\n",
        "print(\"pdb_ligandName.shape:\", pdb_ligandName.shape)\n",
        "\n",
        "print(\"pdb_proteinName_small.shape:\", pdb_proteinName_small.shape)\n",
        "print(\"pdb_affinity_small.shape:\", pdb_affinity_small.shape)\n",
        "print(\"pdb_ligandName_small.shape:\", pdb_ligandName_small.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pdb_proteinName.shape: (17679,)\n",
            "pdb_affinity.shape: (17679,)\n",
            "pdb_ligandName.shape: (17679,)\n",
            "pdb_proteinName_small.shape: (193,)\n",
            "pdb_affinity_small.shape: (193,)\n",
            "pdb_ligandName_small.shape: (193,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR1G-0oxQSIx"
      },
      "source": [
        " #### 3D Representation 1: Contact map\n",
        "\n",
        "We get the list of residues in each protein and treat that as a single chain to calculate the contact map of this protein. After flattened, the length of the contact map vector should be **O(residue_count * residue_count)**, which is too big for some proteins with large number of residues. Hence, we tried to shrink it by collapsing the 3D coordinates a group of neighboring residues into a single one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTIiSHZRSFYF"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def filter_residues(residues):\n",
        "  new_residues = []\n",
        "  for res in residues:\n",
        "    if 'CA' in res:\n",
        "      new_residues.append(res)\n",
        "  return new_residues\n",
        "\n",
        "def calc_residue_dist_by_residues(residue_one, residue_two) :\n",
        "    \"\"\"Returns the C-alpha distance between two residues\"\"\"\n",
        "    diff_vector  = residue_one[\"CA\"].coord - residue_two[\"CA\"].coord\n",
        "    return np.sqrt(np.sum(diff_vector * diff_vector))\n",
        "\n",
        "#Essentially the Euclidean distance between 2 coords\n",
        "def calc_residue_dist_by_coords(coord1, coord2) :\n",
        "    \"\"\"Returns the C-alpha distance between two residues\"\"\"\n",
        "    diff_vector  = coord1 - coord2\n",
        "    # return np.sqrt(np.sum(diff_vector * diff_vector))\n",
        "    return np.linalg.norm(diff_vector)\n",
        "\n",
        "\n",
        "\n",
        "# We expect residue_list1 and residue_list2 to be the same thing: residue list of one protein. Hence the result should be a symmetric matrix, and thus we decided to only calculate the independent entries.\n",
        "def calc_dist_matrix_by_residues(residue_list1) :\n",
        "    \"\"\"Returns a matrix of C-alpha distances between two chains\"\"\"\n",
        "    # answer = np.zeros((len(residue_list1), len(chain_two)), np.float)\n",
        "    answer = []\n",
        "    for row, residue_one in enumerate(residue_list1) :\n",
        "        for col, residue_two in enumerate(residue_list1[row+1:]) :\n",
        "            answer.append(calc_residue_dist(residue_one, residue_two))\n",
        "    return answer\n",
        "\n",
        "def calc_dist_matrix_by_coords(coord_list1) :\n",
        "    \"\"\"Returns a matrix of C-alpha distances between two chains\"\"\"\n",
        "    # answer = np.zeros((len(residue_list1), len(chain_two)), np.float)\n",
        "    answer = []\n",
        "    for row, coord1 in enumerate(coord_list1) :\n",
        "        for col, coord2 in enumerate(coord_list1[row+1:]) :\n",
        "            answer.append(calc_residue_dist_by_coords(coord1, coord2))\n",
        "    return answer\n",
        "\n",
        "\n",
        "# Returns the contact map for one protein and prints the info - slow (for demonstration purpose)\n",
        "# It does the shrinking by shrinking_ratio within each chain (coords from end of chain1 and start of chain 2 will NOT be collapsed together).\n",
        "def get_protein_contact_map_slow(proteinName, shrinking_ratio=20):\n",
        "  structure=MMTFParser.get_structure_from_url(proteinName)\n",
        "  all_chains =structure[0]\n",
        "  \n",
        "  #Get 1.residues: list of all residues in the protein\n",
        "#      2.CA_coord_list has length equal num of chains. \n",
        "  residues = []\n",
        "  CA_coord_list = []\n",
        "\n",
        "  for idx_chain, chain in enumerate(all_chains):\n",
        "    CA_curr_chain = []\n",
        "    for idx_residue, residue in enumerate(chain):\n",
        "      residues.append(residue)\n",
        "      if 'CA' in residue:\n",
        "        CA_curr_chain.append(residue['CA'].coord)\n",
        "    CA_coord_list.append(CA_curr_chain)\n",
        "    print(\"Chain with id %d has %d residues in total, and %d residues with CA.\" % (idx_chain, len(chain), len(CA_curr_chain)))\n",
        "  print(\"\\nCount of residues: \", len(residues))\n",
        "  residues = filter_residues(residues)\n",
        "  print(\"Count of residues with CA: \", len(residues))\n",
        "\n",
        "\n",
        "  # Get coord_list: collapsing list of CA coords of all resiudues with CA in the protein\n",
        "  coord_list = []\n",
        "  for CA_curr_chain in CA_coord_list:\n",
        "    cutting_points = list(np.arange(0,len(CA_curr_chain),shrinking_ratio))\n",
        "    cutting_points.append(len(CA_curr_chain))\n",
        "    for i, start in enumerate(cutting_points[:-1]):\n",
        "      end = cutting_points[i+1]\n",
        "      coord_list.append(np.mean(CA_curr_chain[start:end],axis=0))\n",
        "  print(\"After shrinking, len(coord_list) = \", len(coord_list))\n",
        "\n",
        "\n",
        "  # Calculate contact_map: pair wise distance of the coords\n",
        "  contact_map = []\n",
        "  for coord1 in coord_list:\n",
        "    for coord2 in coord_list:\n",
        "      dist = calc_residue_dist_by_coord(coord1, coord2)\n",
        "      contact_map.append(dist)\n",
        "  print(\"len(contact_map) = \", len(contact_map), end=\"\\n\\n\")\n",
        "  return contact_map\n",
        "\n",
        "\n",
        "\n",
        "# Returns the contact map for one protein and does not print the info - fast\n",
        "# It does not do the exact shrinking within each chain (That is, coords from end of chain1 and start of chain 2 will be collapsed together).\n",
        "def get_protein_contact_map(proteinName, shrinking_ratio=20):\n",
        "  structure=MMTFParser.get_structure_from_url(proteinName)\n",
        "  all_chains =structure[0]\n",
        "  \n",
        "  #Get CA_coord_list: list of CA coords of all residues with CA\n",
        "  CA_coord_list = []\n",
        "  for idx_chain, chain in enumerate(all_chains):\n",
        "    for idx_residue, residue in enumerate(chain):\n",
        "      if 'CA' in residue:\n",
        "        CA_coord_list.append(residue['CA'].coord)\n",
        "  # print(\"len(CA_coord_list) = \", len(CA_coord_list))\n",
        "\n",
        "\n",
        "  # Get coord_list: collapsing list of CA coords of all resiudues with CA in the protein\n",
        "  coord_list = []\n",
        "  cutting_points = list(np.arange(0,len(CA_coord_list),shrinking_ratio))\n",
        "  cutting_points.append(len(CA_curr_chain))\n",
        "  for i, start in enumerate(cutting_points[:-1]):\n",
        "    end = cutting_points[i+1]\n",
        "    coord_list.append(np.mean(CA_coord_list[start:end],axis=0))\n",
        "  # print(\"len(coord_list) = \", len(coord_list))\n",
        "\n",
        "  # Calculate contact_map: pair wise distance of the coords\n",
        "  contact_map = []\n",
        "  contact_map = calc_dist_matrix_by_coords(coord_list)\n",
        "  # print(\"len(contact_map) = \", len(contact_map))\n",
        "\n",
        "  return contact_map\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "6QKcBOfx5lY9",
        "outputId": "67ad56c1-15f6-4c3a-dee8-2c794430f5ef"
      },
      "source": [
        "def foo(proteinName):\n",
        "  all_chains=MMTFParser.get_structure_from_url(proteinName)[0]\n",
        "  CA_coord_list = []\n",
        "  for idx_chain, chain in enumerate(all_chains):\n",
        "    for idx_residue, residue in enumerate(chain):\n",
        "      if 'CA' in residue:\n",
        "        CA_coord_list.append(residue['CA'].coord)\n",
        "\n",
        "  return \n",
        "\n",
        "for i in range(len(pdb_proteinName)):\n",
        "  foo(pdb_proteinName[i])\n",
        "  \n",
        "  if i %10==0:\n",
        "    print(i, end=\",\")\n",
        "  \n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-e67a24628f37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdb_proteinName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mfoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdb_proteinName\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-e67a24628f37>\u001b[0m in \u001b[0;36mfoo\u001b[0;34m(proteinName)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproteinName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mall_chains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMMTFParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_structure_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproteinName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mCA_coord_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0midx_chain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_chains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx_residue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MMTFParser' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB_VxEFV5lR6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_OK8cQJ5lKk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7g2qraA5lD5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jyf4LM0hgZMe"
      },
      "source": [
        "map = get_protein_contact_map_slow(pdb_proteinName[0],shrinking_ratio=SHRINKING_RATIO)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwP4uZBIO15s"
      },
      "source": [
        "# Get contact maps for all proteins:\n",
        "\n",
        "SHRINKING_RATIO = 20 #after exploring, 20 is a good SHRINKING_RATIO to shrink the contact maps into reasonable lengths\n",
        "contact_maps = []\n",
        "lens = []\n",
        "for i in range(len(pdb_proteinName)):\n",
        "  map = get_protein_contact_map(pdb_proteinName_small[i],shrinking_ratio=SHRINKING_RATIO)\n",
        "  contact_maps.append(map)\n",
        "  lens.append(len(map))\n",
        "  \n",
        "  if i %10==0:\n",
        "    print(i, end=\",\")\n",
        "  \n",
        "print(np.min(lens), np.max(lens))\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7T0ZsUGjWRy"
      },
      "source": [
        "# Now do cutting/padding on the data so they have equal length:\n",
        "#TODO: use the code from miniproject (how they got 1000-long vector for proteins)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KzHP22jZ81f"
      },
      "source": [
        "Note: After optimizing the code as much as we can, getting the contact maps of proteins is still slow because of some unavoidable calculations. Each protein takes about **1 sec** to calculate its contact map, which representation takes **???#TODO sec** to calculate for all 17680 proteins.\n",
        "\n",
        "Hence, we will only sample the core binding triples from 'pdbbind_core_df.csv' and we will compare performances of Representation 1 and Representation 2 on this dataset.\n",
        "\n",
        "For the general dataset 'PDBbind2019PLdata.csv',Representation 2 is the only option we have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsZB8uY0k7H2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3AugtrwREe3"
      },
      "source": [
        " #### 3D Representation 2: Distances of residues from the center.\n",
        "We assume a center for the protein, which is calculated by averaging the 3D coordinates of the residues. Then we use the distances of residues from this center as our 3D representation of this protein. One advantage of this representation is that the final length is of order **O(residue_count)**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl-V-z8XRDS6"
      },
      "source": [
        "def get_center_distances(proteinName, printInfo=False):\n",
        "  all_chains=MMTFParser.get_structure_from_url(proteinName)[0]\n",
        "  \n",
        "\n",
        "  #Step1: Get CA_coord_list: list of CA coords of all residues with CA\n",
        "  CA_coord_list = []\n",
        "  for idx_chain, chain in enumerate(all_chains):\n",
        "    for idx_residue, residue in enumerate(chain):\n",
        "      if 'CA' in residue:\n",
        "        CA_coord_list.append(residue['CA'].coord)\n",
        "  if printInfo:\n",
        "    print(\"len(CA_coord_list) = \", len(CA_coord_list))\n",
        "\n",
        "  #Step2: average coords to get center\n",
        "  center = np.mean(CA_coord_list, axis=0)\n",
        "  if printInfo:\n",
        "    print(\"center = \", center)\n",
        "\n",
        "\n",
        "  #Step3: calculate distances from the center\n",
        "  distances = []\n",
        "  for coord in CA_coord_list:\n",
        "    dist = calc_residue_dist_by_coords(center, coord)#Essentially the Euclidean distance between 2 coords\n",
        "    distances.append(dist)\n",
        "  if printInfo:\n",
        "    print(\"len(distances) = \", len(distances))\n",
        "\n",
        "  return distances\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0rTDWWwi3dY"
      },
      "source": [
        "d0 = get_center_distances(pdb_proteinName[0], printInfo=True)\n",
        "print()\n",
        "d1 = get_center_distances(pdb_proteinName[1], printInfo=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgAx2YFohBuF"
      },
      "source": [
        "# Get contact maps for all proteins:\n",
        "\n",
        "SHRINKING_RATIO = 20 #after exploring, 20 is a good SHRINKING_RATIO to shrink the contact maps into reasonable lengths\n",
        "distances = []\n",
        "lens = []\n",
        "for i in range(len(pdb_proteinName)):\n",
        "  d = get_center_distances(pdb_proteinName[i])\n",
        "  distances.append(d)\n",
        "  lens.append(len(d))\n",
        "  \n",
        "  if i %10==0:\n",
        "    print(i, end=\",\")\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOQTBp7Biw_Q"
      },
      "source": [
        "print(np.min(lens), np.max(lens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cNfNwXGjx8v"
      },
      "source": [
        "# Now do cutting/padding on the data so they have equal length:\n",
        "#TODO: use the code from miniproject (how they got 1000-long vector for proteins)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc16K36GXF5c"
      },
      "source": [
        "### 2.2 Parsing KIBA data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5hHvqr7XSaI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu_V0gH7SFYK"
      },
      "source": [
        "# for converting protein sequence to categorical format\n",
        "seq_voc = \"ABCDEFGHIKLMNOPQRSTUVWXYZ\"\n",
        "seq_dict = {v:i for i,v in enumerate(seq_voc)}\n",
        "seq_dict_len = len(seq_dict)\n",
        "max_seq_len = 1000   # Note that all protein data will have the same length 1000 \n",
        "\n",
        "def seq_to_cat(prot):\n",
        "    x = np.zeros(max_seq_len)\n",
        "    for i, ch in enumerate(prot[:max_seq_len]): \n",
        "        x[i] = seq_dict[ch]\n",
        "    return x  \n",
        "\n",
        "# Concordance index evaluation. XM: given function is too slow, we use concordance_index from lifelines.utils to calculate CI score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_FG7L-Rn1oV"
      },
      "source": [
        "We read in the ligands and the proteins, as well as the binding data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5advqtV7SFYP",
        "scrolled": true
      },
      "source": [
        "fpath = 'data_Drug_target_binding_affinity/data/kiba/'\n",
        "\n",
        "# Read in drugs and proteins\n",
        "drugs_ = json.load(open(fpath + \"ligands_can.txt\"), object_pairs_hook=OrderedDict)\n",
        "drugs = np.array([Chem.MolToSmiles(Chem.MolFromSmiles(d),isomericSmiles=True) for d in drugs_.values()])\n",
        "proteins_ = json.load(open(fpath + \"proteins.txt\"), object_pairs_hook=OrderedDict)\n",
        "proteins = np.array(list(proteins_.values()))\n",
        "\n",
        "# Read in affinity data\n",
        "affinity = np.array(pickle.load(open(fpath + \"Y\",\"rb\"), encoding='latin1'))\n",
        "\n",
        "# Read in train/test fold  \n",
        "train_fold = json.load(open(fpath + \"folds/train_fold_setting1.txt\"))\n",
        "train_fold = [ee for e in train_fold for ee in e ]    \n",
        "'''\n",
        "Here all validation folds are aggregated into training set. \n",
        "If you want to train models with different architectures and/or \n",
        "optimize for model hyperparameters, we encourage you to use 5-fold \n",
        "cross validation as provided here.\n",
        "'''\n",
        "test_fold = json.load(open(fpath + \"folds/test_fold_setting1.txt\"))\n",
        "\n",
        "# Prepare train/test data with fold indices\n",
        "rows, cols = np.where(np.isnan(affinity)==False) \n",
        "drugs_tr = drugs[rows[train_fold]]\n",
        "proteins_tr = np.array([seq_to_cat(p) for p in proteins[cols[train_fold]]])\n",
        "affinity_tr = affinity[rows[train_fold], cols[train_fold]]\n",
        "\n",
        "drugs_ts = drugs[rows[test_fold]]\n",
        "proteins_ts = np.array([seq_to_cat(p) for p in proteins[cols[test_fold]]])\n",
        "affinity_ts = affinity[rows[test_fold], cols[test_fold]]    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjRYFfBGiFHy"
      },
      "source": [
        "NBITS = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWy-vFezn6_A"
      },
      "source": [
        "print('XM: Now each protein is represented as a 1000 float array, but each drug is still SMILES string, we convert it to ECFP vectors')\n",
        "\n",
        "# Convert to ECFP fingerprint\n",
        "drugs_tr_ECFP = []\n",
        "for i,drug in enumerate(drugs_tr):\n",
        "  molecule = MolFromSmiles(drug)\n",
        "  ECFP2 = Chem.GetMorganFingerprintAsBitVect(molecule, 2, nBits=NBITS).ToBitString()\n",
        "  ECFP2arr = np.array(list(map(int, ECFP2)))\n",
        "  drugs_tr_ECFP.append(ECFP2arr)\n",
        "  if i % 1000==0:\n",
        "    print(i, end=\",\")\n",
        "\n",
        "drugs_tr_ECFP = np.array(drugs_tr_ECFP)\n",
        "print(drugs_tr_ECFP.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ur6boWg-4NP"
      },
      "source": [
        "print(drugs_tr_ECFP.shape)\n",
        "print(proteins_tr.shape)\n",
        "print(affinity_tr.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyTMcZcE_Cbp"
      },
      "source": [
        "drugs_ts_ECFP = []\n",
        "for i,drug in enumerate(drugs_ts):\n",
        "  molecule = MolFromSmiles(drug)\n",
        "  ECFP2 = Chem.GetMorganFingerprintAsBitVect(molecule, 2, nBits=NBITS).ToBitString()\n",
        "  ECFP2arr = np.array(list(map(int, ECFP2)))\n",
        "  drugs_ts_ECFP.append(ECFP2arr)\n",
        "  if i % 1000==0:\n",
        "    print(i, end=\",\")\n",
        "\n",
        "drugs_ts_ECFP = np.array(drugs_ts_ECFP)\n",
        "print(drugs_ts_ECFP.shape)\n",
        "\n",
        "\n",
        "print(drugs_tr_ECFP.shape)\n",
        "print(proteins_ts.shape)\n",
        "print(affinity_ts.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYNiJ6zA-1AE"
      },
      "source": [
        "pair_tr=[]\n",
        "N=len(drugs_tr_ECFP)\n",
        "print(\"N = \", N)\n",
        "\n",
        "for i in range(N):\n",
        "  new_pair_tr=np.concatenate((proteins_tr[i], drugs_tr_ECFP[i]))\n",
        "  # new_pair_tr=np.array([proteins_tr[i], drugs_tr_ECFP[i]])\n",
        "  pair_tr.append(new_pair_tr)\n",
        "pair_tr = np.array(pair_tr)\n",
        "print(\"pair_tr(concatenated).shape = \", pair_tr.shape)\n",
        "print(\"affinity_tr.shape = \", affinity_tr.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egrHV4hhtSyY"
      },
      "source": [
        "pair_ts=[]\n",
        "N=len(drugs_ts_ECFP)\n",
        "print(\"N = \", N)\n",
        "\n",
        "for i in range(N):\n",
        "  new_pair_ts=np.concatenate((proteins_ts[i], drugs_ts_ECFP[i]))\n",
        "  pair_ts.append(new_pair_ts)\n",
        "pair_ts = np.array(pair_ts)\n",
        "print(\"pair_ts(concatenated).shape = \", pair_ts.shape)\n",
        "print(\"affinity_ts.shape = \", affinity_ts.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRXYpyYVSFYS"
      },
      "source": [
        "## 3. Train a model on KIBA data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qODLvdvESFYT"
      },
      "source": [
        "\n",
        "# First we define a keras sequential model\n",
        "keras_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(1000+NBITS,)),\n",
        "    # tf.keras.layers.Flatten(input_shape=(2,)), (XM: actually our concatenation has done the flatten step) \n",
        "    tf.keras.layers.Dense(1024, activation='relu'),\n",
        "    # tf.keras.layers.Dense(16, activation='relu'),\n",
        "    # tf.keras.layers.Dropout(rate=0.2),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "keras_model2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(1000+NBITS,)),\n",
        "    # tf.keras.layers.Flatten(input_shape=(2,)), (XM: actually our concatenation has done the flatten step) \n",
        "    tf.keras.layers.Dense(1024, activation='relu'),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dense(4, activation='relu'),\n",
        "    tf.keras.layers.Dropout(rate=0.2),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# loss_fn = tf.keras.losses.MSE()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AYHkaWixRri"
      },
      "source": [
        "def train_test_model(model, pair_tr, affinity_tr, pair_ts, affinity_ts, epochs=20, calculateCI=True):\n",
        "  model.compile(optimizer='adam',     # This object specifies the training procedure\n",
        "              loss= 'mean_squared_error',         # The function to minimize during optimization\n",
        "              metrics=['mse']) # Used to monitor training. XM: metrics=[ci] gives bug (may due to the numpy calculation inside of ci(y,f), Google says we might need to downgrade numpy to lower than 1.20: pip install numpy==1.19. However, let's just calculate CI by ourselves in the end\n",
        "\n",
        "  model.fit(pair_tr, affinity_tr, epochs=epochs)\n",
        "\n",
        "  print(\"Evaluation:\")\n",
        "  print('training MSE:', model.evaluate(pair_tr, affinity_tr))\n",
        "  print('test MSE:', model.evaluate(pair_ts, affinity_ts))\n",
        "  if calculateCI:\n",
        "    print('test CI:', ci(affinity_ts, model.predict(pair_ts)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivq3v-atLzB0"
      },
      "source": [
        "train_test_model(keras_model, pair_tr, affinity_tr, pair_ts, affinity_ts, epochs=20, calculateCI=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCp4KeNvSgXg"
      },
      "source": [
        "train_test_model(keras_model2, pair_tr, affinity_tr, pair_ts, affinity_ts, epochs=20, calculateCI=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CT2TZWIG4jN"
      },
      "source": [
        "####  Try using deepchem keras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajD3NMMiNWFu"
      },
      "source": [
        "\n",
        "def train_test_model_dc(model_keras, pair_tr, affinity_tr, pair_ts, affinity_ts, epochs=20, calculateCI=True):\n",
        "  # train_dataset = dc.data.NumpyDataset(X=pair_tr, y=affinity_tr.reshape(len(affinity_tr), 1))\n",
        "  # test_dataset = dc.data.NumpyDataset(X=pair_ts, y=affinity_ts.reshape(len(affinity_ts), 1))\n",
        "  # dc_model = dc.models.KerasModel(model_keras, dc.models.losses.L2Loss()) # we pass the model and a dc loss funciton\n",
        "  \n",
        "  # dc_model.fit(train_dataset, nb_epoch=epochs)\n",
        "  # metric = dc.metrics.Metric(dc.metrics.mean_squared_error)\n",
        "  # print(\"Evaluation:\")\n",
        "  # print('training MSE:', dc_model.evaluate(train_dataset, [metric]))\n",
        "  # print('test MSE:',dc_model.evaluate(test_dataset, [metric]))\n",
        "  # if calculateCI:\n",
        "  #   print('test CI:', ci(affinity_ts, dc_model.predict(test_dataset)))\n",
        "  \n",
        "  #TODO: Install deepchem takes too long every time. Here are temporarily usee keras NN for training. \n",
        "  # After finishing all coding. Comment out the code below and use the code above!\n",
        "\n",
        "  return train_test_model(model_keras, pair_tr, affinity_tr, pair_ts, affinity_ts, epochs=epochs, calculateCI=calculateCI)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgxbowmXG3fn"
      },
      "source": [
        "train_test_model_dc(keras_model2, pair_tr, affinity_tr, pair_ts, affinity_ts, epochs=20, calculateCI=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K67QchKnSFYZ"
      },
      "source": [
        "[The original paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4364066/) obtained CI score of 0.782 and MSE of 0.411, and [random forest model](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5395521/) got 0.836 and 0.222. Can you beat them?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGmlZlAn1KpB"
      },
      "source": [
        "Miniproject notesï¼š\n",
        "* Protein 1000; Drug 2048; tf Keras:\n",
        "\n",
        " **Training: MSE=0.704 (epoch=10); Test: MSE=0.679, CI=0.5**\n",
        "\n",
        "* Protein 1000; Drug 1000; tf Keras:\n",
        "   \n",
        "   **Training: MSE=0.390 (epoch=20); Test: MSE=0.391, CI=0.765**\n",
        "\n",
        "* Try more complicated NN (keras_model2):\n",
        "\n",
        "  **Training: MSE=0.356 (epoch=20); Test: MSE=0.359, CI=0.773**\n",
        "\n",
        "* Protein 1000; Drug 1000; dc model with (keras_model2):\n",
        "   \n",
        "   **Training: MSE=0.303 (epoch=20); Test: MSE=0.316, CI=0.782**\n",
        "\n",
        "* TODO: COVID-19 prediction (HIV score seems not very good??)\n",
        "* TODO: Julia + Decision Tree\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my0BqyOkSFYa"
      },
      "source": [
        "## 4. Use  your model on COVID-19 protease"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkmmMhjOSFYc"
      },
      "source": [
        "Now, use your trained model to identify drugs that could be used as COVID-19 protease inhibitors from these 2111 drugs in the dataset. The sequence of the protease is provided in `6Y84_A.fasta.txt`. You might want to first predict a binding affinity of Ritonavior, a well known HIV drug that binds to HIV protease, to get the sense of a good binding score for this task. SMILES of Ritonavior is provided below.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN0eBnfUZloQ"
      },
      "source": [
        "def protein2vec(protein_seq):\n",
        "  return seq_to_cat(protein_seq)\n",
        "\n",
        "def drug2vec(drug_smiles, nBits=NBITS):\n",
        "  molecule = MolFromSmiles(drug_smiles)\n",
        "  ECFP2 = Chem.GetMorganFingerprintAsBitVect(molecule, 2, nBits).ToBitString()#XM: default nBits=2048\n",
        "  drug_ECFP = np.array(list(map(int, ECFP2)))\n",
        "  return drug_ECFP\n",
        "\n",
        "\n",
        "# train_dataset = dc.data.NumpyDataset(X=pair_tr, y=affinity_tr.reshape(len(affinity_tr), 1))\n",
        "# dc_model = dc.models.KerasModel(keras_model, dc.models.losses.L2Loss()) # we pass the model and a dc loss funciton\n",
        "# dc_model.fit(train_dataset, nb_epoch=20)\n",
        "# print(\"dc_model is ready!\")\n",
        "\n",
        "# def protein_drug_predict_dc(protein_vec, drug_vec):#XM: this doesn't work well on HIV\n",
        "#   protein_drug_pair = np.concatenate((protein_vec, drug_vec))\n",
        "#   protein_drug_pair = protein_drug_pair.reshape((1, len(protein_drug_pair)))\n",
        "\n",
        "#   test_dataset = dc.data.NumpyDataset(X=protein_drug_pair)\n",
        "#   return dc_model.predict(test_dataset)[0][0]\n",
        "\n",
        "keras_model.compile(optimizer='adam',  \n",
        "              loss= 'mean_squared_error', \n",
        "              metrics=['mse']) \n",
        "keras_model.fit(pair_tr, affinity_tr, epochs=20)\n",
        "print(\"keras_model is ready!\")\n",
        "\n",
        "keras_model2.compile(optimizer='adam',  \n",
        "              loss= 'mean_squared_error', \n",
        "              metrics=['mse']) \n",
        "keras_model2.fit(pair_tr, affinity_tr, epochs=20)\n",
        "print(\"keras_model2 is ready!\")\n",
        "\n",
        "def protein_drug_predict(protein_vec, drug_vec, model):\n",
        "  protein_drug_pair = np.concatenate((protein_vec, drug_vec))\n",
        "  protein_drug_pair = protein_drug_pair.reshape((1, len(protein_drug_pair)))\n",
        "  \n",
        "  return model.predict([protein_drug_pair])[0][0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj11JSsM2GfE"
      },
      "source": [
        "# HIV: https://www.uniprot.org/uniprot/O90777; https://www.uniprot.org/uniprot/O90777/protvista\n",
        "HIV_prot = 'PQVTLWQRPIVTIKIGGQLKEALLDTGADDTVLEEMSLPGKWKPKMIGGIGGFIKVRQYDQVSIEICGHKAIGTVLIGPTPVNIIGRNLLTQLGCTLNF'\n",
        "HIV_vec = seq_to_cat(HIV_prot)\n",
        "print(HIV_vec.shape)\n",
        "\n",
        "ritonavior = 'CC(C)C1=NC(=CS1)CN(C)C(=O)NC(C(C)C)C(=O)NC(CC2=CC=CC=C2)CC(C(CC3=CC=CC=C3)NC(=O)OCC4=CN=CS4)O'\n",
        "ritonavior_ECFP = drug2vec(ritonavior)\n",
        "print(ritonavior_ECFP.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ_q2-bRWVsM"
      },
      "source": [
        "print(\"HIV-Ritonavior binding score:\")\n",
        "HIV_Ritonavior_score = protein_drug_predict(HIV_vec, ritonavior_ECFP, keras_model2)\n",
        "HIV_Ritonavior_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFg4eHP1YLer"
      },
      "source": [
        "print(\"HIV-AnotherDrug binding scores:\")\n",
        "\n",
        "HIV_affinities = []\n",
        "for i,drug in enumerate(drugs):\n",
        "  drug_ECFP = drug2vec(drug)\n",
        "  aff = protein_drug_predict(HIV_vec, drug_ECFP, keras_model2)\n",
        "  HIV_affinities.append(aff)\n",
        "\n",
        "HIV_affinities = np.array(HIV_affinities)\n",
        "print(HIV_affinities[:20])\n",
        "print(sorted(np.array(list(set(HIV_affinities)))))\n",
        "\n",
        "print(\"\\n\\nHow many drugs have better bindings with HIV, compared to Ritonavior?\")\n",
        "print(len(HIV_affinities[HIV_affinities > HIV_Ritonavior_score]) / len(HIV_affinities))\n",
        "print(\"How many drugs have equal bindings with HIV, compared to Ritonavior?\")\n",
        "print(len(HIV_affinities[HIV_affinities == HIV_Ritonavior_score]) / len(HIV_affinities))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7Aon_2Pcvrw"
      },
      "source": [
        "* When using 1000-vector for drugs:\n",
        "\n",
        "   It seems almost 70% out of the 2111 drugs bind better with HIV, compared to Ritonavior. Thus our model might not be good enough?\n",
        "\n",
        "* When using 1000-vector for drugs:\n",
        "\n",
        "  Indeed Ritonavior has the highest binding score, but 94% of the 2111 drugs have the same binding score...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWqUOWkTP9oR"
      },
      "source": [
        "COVID_list = []\n",
        "with open('data_Drug_target_binding_affinity/data/6Y84_A.fasta.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        COVID_list.append(line[:-1])\n",
        "\n",
        "COVID_seq = ''\n",
        "for s in COVID_list[1:]:\n",
        "  COVID_seq += s\n",
        "print(\"COVID-19 amino acids sequence: \", COVID_seq)\n",
        "COVID_vec = seq_to_cat(COVID_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNZ6n7PvenTA"
      },
      "source": [
        "print(\"COVID-Drug binding scores:\")\n",
        "COVID_affinities = []\n",
        "for i,drug in enumerate(drugs):\n",
        "  drug_ECFP = drug2vec(drug)\n",
        "  aff = protein_drug_predict(COVID_vec, drug_ECFP, keras_model2)\n",
        "  COVID_affinities.append(aff)\n",
        "\n",
        "COVID_affinities = np.array(COVID_affinities)\n",
        "print(COVID_affinities[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlOUQc2Xepx4"
      },
      "source": [
        "print(\"Below are the Top 5 drug candidates for COVID-19:\")\n",
        "print(\"Affinitiy score       Drug smiles: \")\n",
        "for idx in np.argsort(COVID_affinities)[-5:]:\n",
        "  drug_smiles = drugs[idx]\n",
        "  aff_score = COVID_affinities[idx]\n",
        "  print(aff_score, \"         \",  drug_smiles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aFgkgCWFaIl"
      },
      "source": [
        "## 5. Dimension reduction.\n",
        "\n",
        "Now because the number of data samples is large. It takes quite long to run NN without GPU. Hence, let's investigate how well the reduced data would perform in our NN model. \n",
        "\n",
        "#### 5.1. Reduction by Autoencoder\n",
        "###### 5.1.1 Try use 2 Autoencoders for pair_tr and pair_ts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc1zekZ3GG8D"
      },
      "source": [
        "#### 5.2. Reduction by PCA\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k95PcmDbOLUj"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=128)\n",
        "pair_tr_pca = pca.fit_transform(pair_tr)\n",
        "print(\"pair_tr_pca shape:\", pair_tr_pca.shape)\n",
        "print(\"Explained how much variance:\", pca.explained_variance_ratio_.cumsum()[-1])\n",
        "\n",
        "pca = PCA(n_components=128)\n",
        "pair_ts_pca = pca.fit_transform(pair_ts)\n",
        "print(\"\\npair_ts_pca shape:\", pair_ts_pca.shape)\n",
        "print(\"Explained how much variance:\", pca.explained_variance_ratio_.cumsum()[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sVlCLYKPkNW"
      },
      "source": [
        "train_test_model(keras_model2, pair_tr_pca, affinity_tr, pair_ts_pca, affinity_ts, epochs=20, calculateCI=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFRcNb5ZawVp"
      },
      "source": [
        "We see after reduction by Autoencoder or PCA, our NN still gives relatively good results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfW-Ug8sSIbe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}